/**
 * IRæ–‡æ¡£å¤„ç†å™¨ V2 - é…åˆæŒä¹…åŒ–ç´¢å¼•æœåŠ¡
 * è§£å†³åŸç³»ç»Ÿç´¢å¼•å­˜å‚¨åœ¨å†…å­˜ä¸­çš„é—®é¢˜
 */

import { nanoid } from "nanoid";
import {
  getIRIndexService,
  DatabaseDocument,
  DatabaseChunk,
  DatabaseTerm,
  DatabasePosting,
} from "../services/ir-index-service";

// ================ å¢å¼ºçš„æ–‡æ¡£æ¥å£ ================

export interface IRDocumentV2 {
  id: string;
  fileName: string;
  fileType: string;
  size: number;

  // å…ƒæ•°æ®
  title?: string;
  url?: string;
  language?: string;

  // æ—¶é—´æˆ³
  uploadedAt: string;
  processedAt?: string;
  lastModified?: string;

  // å¤„ç†çŠ¶æ€
  status: "processing" | "completed" | "error";
  error?: string;

  // ç»Ÿè®¡ä¿¡æ¯
  totalTokens: number;
  chunkCount: number;
  termStats: {
    totalTerms: number;
    uniqueTerms: number;
    avgTermsPerChunk: number;
  };
}

export interface IRChunkV2 {
  id: string;
  docId: string;
  chunkIndex: number;
  content: string;
  tokenCount: number;

  // ä½ç½®ä¿¡æ¯
  startOffset: number;
  endOffset: number;

  // ç»“æ„ä¿¡æ¯
  isTitle: boolean;
  isTableHeader: boolean;
  sectionTitle?: string;
  page?: number;

  // è¯­è¨€å’Œè´¨é‡
  language?: string;
  confidence?: number;

  // è¯æ±‡ä¿¡æ¯
  terms: string[];
  termFreqs: Map<string, number>;

  createdAt: string;
}

// ================ å¤„ç†ç»“æœæ¥å£ ================

export interface ProcessingResult {
  document: IRDocumentV2;
  chunks: IRChunkV2[];
  terms: DatabaseTerm[];
  postings: DatabasePosting[];
  indexStats: {
    totalDocuments: number;
    totalChunks: number;
    totalTerms: number;
    uniqueTerms: number;
    avgDocumentLength: number;
    avgChunkLength: number;
  };
}

// ================ å¢å¼ºçš„æ–‡æ¡£å¤„ç†å™¨ ================

export class IRDocumentProcessorV2 {
  private static instance: IRDocumentProcessorV2;

  // å¤„ç†å‚æ•°
  private readonly chunkParams = {
    minTokens: 400,
    maxTokens: 800,
    overlapRatio: 0.15,
  };

  // å­—æ®µæƒé‡å‚æ•°
  private readonly fieldWeights = {
    title: 2.0,
    content: 1.0,
    tableHeader: 1.5,
  };

  // åœç”¨è¯é›†åˆ
  private readonly stopWords = new Set([
    // English stop words
    "the",
    "a",
    "an",
    "and",
    "or",
    "but",
    "in",
    "on",
    "at",
    "to",
    "for",
    "of",
    "with",
    "by",
    "is",
    "are",
    "was",
    "were",
    "be",
    "been",
    "have",
    "has",
    "had",
    "do",
    "does",
    "did",
    "will",
    "would",
    "should",
    "could",
    "can",
    "may",
    "might",
    "must",
    "shall",
    // Chinese stop words
    "çš„",
    "äº†",
    "åœ¨",
    "æ˜¯",
    "æˆ‘",
    "æœ‰",
    "å’Œ",
    "å°±",
    "ä¸",
    "äºº",
    "éƒ½",
    "ä¸€",
    "ä¸€ä¸ª",
    "ä¸Š",
    "ä¹Ÿ",
    "å¾ˆ",
    "åˆ°",
    "è¯´",
    "è¦",
    "å»",
    "ä½ ",
    "ä¼š",
    "ç€",
    "æ²¡æœ‰",
    "çœ‹",
    "å¥½",
    "è‡ªå·±",
    "è¿™",
  ]);

  // åŒä¹‰è¯ç»„
  private readonly synonymGroups = new Map<string, string[]>([
    ["äº§å“", ["å•†å“", "ç‰©å“", "åˆ¶å“"]],
    ["æ ‡å‡†", ["è§„èŒƒ", "å‡†åˆ™", "è¦æ±‚", "è§„å®š"]],
    ["è´¨é‡", ["å“è´¨", "è´¨é‡æ ‡å‡†"]],
    ["åˆ¶ä½œ", ["ç”Ÿäº§", "åŠ å·¥", "åˆ¶é€ "]],
    ["æµç¨‹", ["è¿‡ç¨‹", "ç¨‹åº", "æ­¥éª¤"]],
  ]);

  static getInstance(): IRDocumentProcessorV2 {
    if (!IRDocumentProcessorV2.instance) {
      IRDocumentProcessorV2.instance = new IRDocumentProcessorV2();
    }
    return IRDocumentProcessorV2.instance;
  }

  /**
   * å¤„ç†æ–‡ä»¶å¹¶å­˜å‚¨åˆ°æŒä¹…åŒ–ç´¢å¼•
   */
  async processFileAndIndex(file: File): Promise<ProcessingResult> {
    console.log("ğŸ“ [IR Processor V2] Starting file processing:", file.name);

    // ç¬¬ä¸€æ­¥ï¼šå¤„ç†æ–‡æ¡£
    const document = await this.processDocument(file);

    // ç¬¬äºŒæ­¥ï¼šå¤„ç†æ–‡æœ¬å—
    const text = await this.extractText(file);
    const chunks = await this.processChunks(text, document);

    // ç¬¬ä¸‰æ­¥ï¼šæ„å»ºç´¢å¼•æ•°æ®
    const { terms, postings } = await this.buildIndexData(chunks);

    // ç¬¬å››æ­¥ï¼šè®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    const indexStats = this.calculateIndexStats(document, chunks, terms);

    // ç¬¬äº”æ­¥ï¼šå­˜å‚¨åˆ°æŒä¹…åŒ–ç´¢å¼•æœåŠ¡
    await this.storeToIndexService(
      document,
      chunks,
      terms,
      postings,
      indexStats,
    );

    console.log("âœ… [IR Processor V2] File processing completed:", {
      fileName: document.fileName,
      chunksCount: chunks.length,
      termsCount: terms.length,
      postingsCount: postings.length,
    });

    return {
      document,
      chunks,
      terms,
      postings,
      indexStats,
    };
  }

  /**
   * å¤„ç†æ–‡æ¡£åŸºæœ¬ä¿¡æ¯
   */
  private async processDocument(file: File): Promise<IRDocumentV2> {
    const document: IRDocumentV2 = {
      id: nanoid(),
      fileName: file.name,
      fileType: file.type,
      size: file.size,
      title: this.extractTitleFromFileName(file.name),
      language: "zh-cn", // TODO: æ·»åŠ è¯­è¨€æ£€æµ‹
      uploadedAt: new Date().toISOString(),
      status: "processing",
      totalTokens: 0,
      chunkCount: 0,
      termStats: {
        totalTerms: 0,
        uniqueTerms: 0,
        avgTermsPerChunk: 0,
      },
    };

    return document;
  }

  /**
   * å¤„ç†æ–‡æœ¬å—
   */
  private async processChunks(
    text: string,
    document: IRDocumentV2,
  ): Promise<IRChunkV2[]> {
    console.log(
      "ğŸ“„ [IR Processor V2] Processing chunks for:",
      document.fileName,
    );

    const chunks: IRChunkV2[] = [];
    const paragraphs = text.split(/\n\s*\n/);

    let currentChunk = "";
    let currentTokens = 0;
    let chunkIndex = 0;
    let startOffset = 0;

    for (let i = 0; i < paragraphs.length; i++) {
      const paragraph = paragraphs[i].trim();
      if (!paragraph) continue;

      const paragraphTokens = this.estimateTokenCount(paragraph);

      // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°å—
      if (
        currentTokens + paragraphTokens > this.chunkParams.maxTokens &&
        currentChunk
      ) {
        const chunk = await this.createChunk(
          currentChunk,
          document,
          chunkIndex,
          startOffset,
          startOffset + currentChunk.length,
          currentTokens,
        );
        chunks.push(chunk);

        // å‡†å¤‡ä¸‹ä¸€ä¸ªå—ï¼ŒåŒ…å«é‡å 
        const overlapTokens = Math.floor(
          currentTokens * this.chunkParams.overlapRatio,
        );
        const overlapText = this.getLastTokens(currentChunk, overlapTokens);

        currentChunk = overlapText + "\n\n" + paragraph;
        currentTokens = this.estimateTokenCount(currentChunk);
        startOffset = startOffset + currentChunk.length - overlapText.length;
        chunkIndex++;
      } else {
        // æ·»åŠ æ®µè½åˆ°å½“å‰å—
        currentChunk += (currentChunk ? "\n\n" : "") + paragraph;
        currentTokens = this.estimateTokenCount(currentChunk);
      }
    }

    // æ·»åŠ æœ€åä¸€ä¸ªå—
    if (currentChunk.trim()) {
      const chunk = await this.createChunk(
        currentChunk,
        document,
        chunkIndex,
        startOffset,
        startOffset + currentChunk.length,
        currentTokens,
      );
      chunks.push(chunk);
    }

    console.log(`ğŸ“„ [IR Processor V2] Created ${chunks.length} chunks`);
    return chunks;
  }

  /**
   * åˆ›å»ºå•ä¸ªæ–‡æœ¬å—
   */
  private async createChunk(
    content: string,
    document: IRDocumentV2,
    chunkIndex: number,
    startOffset: number,
    endOffset: number,
    tokenCount: number,
  ): Promise<IRChunkV2> {
    const terms = this.extractAndNormalizeTerms(content);
    const termFreqs = this.calculateTermFrequencies(terms);

    const chunk: IRChunkV2 = {
      id: nanoid(),
      docId: document.id,
      chunkIndex,
      content: content.trim(),
      tokenCount,
      startOffset,
      endOffset,
      isTitle: this.isTitle(content),
      isTableHeader: this.isTableHeader(content),
      sectionTitle: this.extractSectionTitle(content),
      language: document.language,
      confidence: this.calculateContentQuality(content),
      terms,
      termFreqs,
      createdAt: new Date().toISOString(),
    };

    return chunk;
  }

  /**
   * æ„å»ºç´¢å¼•æ•°æ®ï¼ˆè¯æ±‡è¡¨å’Œå€’æ’ç´¢å¼•ï¼‰
   */
  private async buildIndexData(chunks: IRChunkV2[]): Promise<{
    terms: DatabaseTerm[];
    postings: DatabasePosting[];
  }> {
    console.log("ğŸ” [IR Processor V2] Building index data...");

    const termMap = new Map<
      string,
      {
        docIds: Set<string>;
        chunkIds: Set<string>;
        totalFreq: number;
        maxFreq: number;
        postings: DatabasePosting[];
      }
    >();

    // å¤„ç†æ¯ä¸ªæ–‡æœ¬å—
    for (const chunk of chunks) {
      for (const [term, freq] of chunk.termFreqs) {
        if (!termMap.has(term)) {
          termMap.set(term, {
            docIds: new Set(),
            chunkIds: new Set(),
            totalFreq: 0,
            maxFreq: 0,
            postings: [],
          });
        }

        const termData = termMap.get(term)!;
        termData.docIds.add(chunk.docId);
        termData.chunkIds.add(chunk.id);
        termData.totalFreq += freq;
        termData.maxFreq = Math.max(termData.maxFreq, freq);

        // åˆ›å»ºå€’æ’ç´¢å¼•é¡¹
        const posting: DatabasePosting = {
          id: nanoid(),
          term,
          docId: chunk.docId,
          chunkId: chunk.id,
          termFreq: freq,
          contentWeight: this.fieldWeights.content,
          createdAt: new Date().toISOString(),
        };

        // æ·»åŠ å­—æ®µæƒé‡
        if (chunk.isTitle) {
          posting.titleWeight = this.fieldWeights.title;
        }
        if (chunk.isTableHeader) {
          posting.tableHeaderWeight = this.fieldWeights.tableHeader;
        }

        termData.postings.push(posting);
      }
    }

    // æ„å»ºè¯æ±‡è¡¨
    const terms: DatabaseTerm[] = [];
    for (const [term, data] of termMap) {
      terms.push({
        term,
        docFreq: data.docIds.size,
        chunkFreq: data.chunkIds.size,
        totalFreq: data.totalFreq,
        avgTermFreq: data.totalFreq / data.chunkIds.size,
        maxTermFreq: data.maxFreq,
        updatedAt: new Date().toISOString(),
      });
    }

    // æ„å»ºå€’æ’ç´¢å¼•
    const postings: DatabasePosting[] = [];
    for (const data of termMap.values()) {
      postings.push(...data.postings);
    }

    console.log(
      `ğŸ” [IR Processor V2] Built index: ${terms.length} terms, ${postings.length} postings`,
    );

    return { terms, postings };
  }

  /**
   * è®¡ç®—ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
   */
  private calculateIndexStats(
    document: IRDocumentV2,
    chunks: IRChunkV2[],
    terms: DatabaseTerm[],
  ) {
    const totalTerms = terms.reduce((sum, term) => sum + term.totalFreq, 0);
    const uniqueTerms = terms.length;
    const totalTokens = chunks.reduce(
      (sum, chunk) => sum + chunk.tokenCount,
      0,
    );

    return {
      totalDocuments: 1, // è¿™é‡Œåªæ˜¯å•ä¸ªæ–‡æ¡£çš„ç»Ÿè®¡
      totalChunks: chunks.length,
      totalTerms,
      uniqueTerms,
      avgDocumentLength: totalTokens,
      avgChunkLength: chunks.length > 0 ? totalTokens / chunks.length : 0,
    };
  }

  /**
   * å­˜å‚¨åˆ°æŒä¹…åŒ–ç´¢å¼•æœåŠ¡
   */
  private async storeToIndexService(
    document: IRDocumentV2,
    chunks: IRChunkV2[],
    terms: DatabaseTerm[],
    postings: DatabasePosting[],
    indexStats: any,
  ): Promise<void> {
    console.log("ğŸ’¾ [IR Processor V2] Storing to index service...");

    const indexService = getIRIndexService();

    // æ›´æ–°æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
    document.totalTokens = chunks.reduce(
      (sum, chunk) => sum + chunk.tokenCount,
      0,
    );
    document.chunkCount = chunks.length;
    document.termStats = {
      totalTerms: terms.reduce((sum, term) => sum + term.totalFreq, 0),
      uniqueTerms: terms.length,
      avgTermsPerChunk:
        chunks.length > 0
          ? terms.reduce((sum, term) => sum + term.totalFreq, 0) / chunks.length
          : 0,
    };
    document.processedAt = new Date().toISOString();
    document.status = "completed";

    // è½¬æ¢ä¸ºæ•°æ®åº“æ ¼å¼
    const dbDocument: DatabaseDocument = {
      id: document.id,
      fileName: document.fileName,
      fileType: document.fileType,
      fileSize: document.size,
      title: document.title,
      language: document.language,
      uploadedAt: document.uploadedAt,
      processedAt: document.processedAt,
      status: document.status,
      error: document.error,
      totalTokens: document.totalTokens,
      chunkCount: document.chunkCount,
      termStats: document.termStats,
    };

    const dbChunks: DatabaseChunk[] = chunks.map((chunk) => ({
      id: chunk.id,
      docId: chunk.docId,
      chunkIndex: chunk.chunkIndex,
      content: chunk.content,
      tokenCount: chunk.tokenCount,
      startOffset: chunk.startOffset,
      endOffset: chunk.endOffset,
      isTitle: chunk.isTitle,
      isTableHeader: chunk.isTableHeader,
      sectionTitle: chunk.sectionTitle,
      page: chunk.page,
      language: chunk.language,
      confidence: chunk.confidence,
      createdAt: chunk.createdAt,
    }));

    // å­˜å‚¨åˆ°ç´¢å¼•æœåŠ¡
    await indexService.addDocument(dbDocument);
    await indexService.addChunks(dbChunks);
    await indexService.addTerms(terms);
    await indexService.addPostings(postings);

    // æ›´æ–°å…¨å±€ç»Ÿè®¡ä¿¡æ¯ï¼ˆè¿™é‡Œéœ€è¦ä»ç´¢å¼•æœåŠ¡è·å–å½“å‰ç»Ÿè®¡å¹¶æ›´æ–°ï¼‰
    const currentStats = await indexService.getIndexStats();
    const updatedStats = {
      totalDocuments: (currentStats?.totalDocuments || 0) + 1,
      totalChunks: (currentStats?.totalChunks || 0) + chunks.length,
      totalTerms: (currentStats?.totalTerms || 0) + indexStats.totalTerms,
      uniqueTerms: (currentStats?.uniqueTerms || 0) + indexStats.uniqueTerms,
      avgDocumentLength: indexStats.avgDocumentLength,
      avgChunkLength: indexStats.avgChunkLength,
      lastUpdated: new Date().toISOString(),
    };

    await indexService.updateIndexStats(updatedStats);

    console.log("âœ… [IR Processor V2] Successfully stored to index service");
  }

  // ================ è¾…åŠ©æ–¹æ³• ================

  /**
   * æå–æ–‡æœ¬å†…å®¹
   */
  private async extractText(file: File): Promise<string> {
    const fileType = file.type;

    if (
      fileType === "application/pdf" ||
      file.name.toLowerCase().endsWith(".pdf")
    ) {
      return await this.extractPdfText(file);
    }

    if (fileType === "text/plain") {
      return await file.text();
    }

    if (fileType === "application/json") {
      const json = JSON.parse(await file.text());
      return this.jsonToText(json);
    }

    if (
      fileType.includes("text/") ||
      file.name.endsWith(".md") ||
      file.name.endsWith(".csv") ||
      file.name.endsWith(".xml")
    ) {
      return await file.text();
    }

    throw new Error(`Unsupported file type: ${fileType}`);
  }

  /**
   * æå–PDFæ–‡æœ¬
   */
  private async extractPdfText(file: File): Promise<string> {
    try {
      const pdfjsLib = await import("pdfjs-dist");
      pdfjsLib.GlobalWorkerOptions.workerSrc = `//cdnjs.cloudflare.com/ajax/libs/pdf.js/${pdfjsLib.version}/pdf.worker.min.js`;

      const arrayBuffer = await file.arrayBuffer();
      const loadingTask = pdfjsLib.getDocument({ data: arrayBuffer });
      const pdf = await loadingTask.promise;

      let fullText = "";
      for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {
        const page = await pdf.getPage(pageNum);
        const textContent = await page.getTextContent();
        const pageText = textContent.items
          .map((item: any) => item.str)
          .join(" ");
        fullText += pageText + "\n";
      }

      if (!fullText || fullText.trim().length === 0) {
        throw new Error(
          "PDF appears to be empty or contains no extractable text",
        );
      }

      return fullText.trim();
    } catch (error) {
      console.error("PDF extraction failed:", error);
      throw new Error(
        `PDF extraction failed: ${
          error instanceof Error ? error.message : "Unknown error"
        }`,
      );
    }
  }

  /**
   * JSONè½¬æ–‡æœ¬
   */
  private jsonToText(obj: any, prefix = ""): string {
    let text = "";

    for (const [key, value] of Object.entries(obj)) {
      const fullKey = prefix ? `${prefix}.${key}` : key;

      if (typeof value === "object" && value !== null) {
        if (Array.isArray(value)) {
          text += `${fullKey}: ${value.join(", ")}\n`;
        } else {
          text += this.jsonToText(value, fullKey);
        }
      } else {
        text += `${fullKey}: ${value}\n`;
      }
    }

    return text;
  }

  /**
   * æå–å¹¶æ ‡å‡†åŒ–è¯æ±‡
   */
  private extractAndNormalizeTerms(text: string): string[] {
    const terms: string[] = [];

    const normalizedText = text
      .toLowerCase()
      .replace(/[^\u4e00-\u9fa5a-zA-Z0-9\s]/g, " ")
      .replace(/\s+/g, " ")
      .trim();

    // è‹±æ–‡è¯æ±‡
    const englishWords = normalizedText.match(/[a-zA-Z]{2,}/g) || [];
    for (const word of englishWords) {
      if (!this.stopWords.has(word)) {
        terms.push(word);
      }
    }

    // æ•°å­—
    const numbers = normalizedText.match(/\d+/g) || [];
    terms.push(...numbers);

    // ä¸­æ–‡è¯æ±‡
    const chineseText = normalizedText.replace(/[a-zA-Z0-9\s]/g, "");
    if (chineseText.length > 0) {
      // å•å­—
      for (let i = 0; i < chineseText.length; i++) {
        const char = chineseText[i];
        if (char && /[\u4e00-\u9fa5]/.test(char) && !this.stopWords.has(char)) {
          terms.push(char);
        }
      }

      // åŒå­—è¯
      for (let i = 0; i < chineseText.length - 1; i++) {
        const bigram = chineseText.substring(i, i + 2);
        if (
          bigram.length === 2 &&
          /^[\u4e00-\u9fa5]{2}$/.test(bigram) &&
          !this.stopWords.has(bigram)
        ) {
          terms.push(bigram);
        }
      }

      // ä¸‰å­—è¯
      for (let i = 0; i < chineseText.length - 2; i++) {
        const trigram = chineseText.substring(i, i + 3);
        if (trigram.length === 3 && /^[\u4e00-\u9fa5]{3}$/.test(trigram)) {
          terms.push(trigram);
        }
      }
    }

    return [...new Set(terms)];
  }

  /**
   * è®¡ç®—è¯é¢‘
   */
  private calculateTermFrequencies(terms: string[]): Map<string, number> {
    const freqs = new Map<string, number>();
    for (const term of terms) {
      freqs.set(term, (freqs.get(term) || 0) + 1);
    }
    return freqs;
  }

  /**
   * ä¼°ç®—tokenæ•°é‡
   */
  private estimateTokenCount(text: string): number {
    const chineseChars = (text.match(/[\u4e00-\u9fa5]/g) || []).length;
    const englishChars = text.length - chineseChars;
    return Math.ceil(chineseChars / 1.5 + englishChars / 4);
  }

  /**
   * è·å–æœ€åNä¸ªtokençš„æ–‡æœ¬ï¼ˆç”¨äºé‡å ï¼‰
   */
  private getLastTokens(text: string, targetTokens: number): string {
    const sentences = text.split(/[ã€‚ï¼ï¼Ÿ.!?]/);
    let result = "";
    let tokens = 0;

    for (let i = sentences.length - 1; i >= 0; i--) {
      const sentence = sentences[i].trim();
      if (!sentence) continue;

      const sentenceTokens = this.estimateTokenCount(sentence);
      if (tokens + sentenceTokens <= targetTokens) {
        result = sentence + "ã€‚" + result;
        tokens += sentenceTokens;
      } else {
        break;
      }
    }

    return result.trim();
  }

  /**
   * ä»æ–‡ä»¶åæå–æ ‡é¢˜
   */
  private extractTitleFromFileName(fileName: string): string {
    return fileName.replace(/\.[^/.]+$/, "").replace(/[-_]/g, " ");
  }

  /**
   * æ£€æµ‹æ˜¯å¦ä¸ºæ ‡é¢˜
   */
  private isTitle(content: string): boolean {
    const lines = content.split("\n");
    const firstLine = lines[0].trim();

    return (
      firstLine.length < 100 &&
      firstLine.length > 2 &&
      lines.length <= 3 &&
      !/[ã€‚ï¼ï¼Ÿ.!?]$/.test(firstLine)
    );
  }

  /**
   * æ£€æµ‹æ˜¯å¦ä¸ºè¡¨æ ¼æ ‡é¢˜
   */
  private isTableHeader(content: string): boolean {
    return /^[^\n]*[|ï½œ]\s*[^\n]*$/.test(content.trim());
  }

  /**
   * æå–ç« èŠ‚æ ‡é¢˜
   */
  private extractSectionTitle(content: string): string | undefined {
    const lines = content.split("\n");
    const firstLine = lines[0].trim();

    if (this.isTitle(content)) {
      return firstLine;
    }

    return undefined;
  }

  /**
   * è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°
   */
  private calculateContentQuality(content: string): number {
    const length = content.length;
    const sentences = content.split(/[ã€‚ï¼ï¼Ÿ.!?]/).length;
    const avgSentenceLength = length / sentences;

    let score = 0.5;

    if (length > 100 && length < 2000) score += 0.2;
    if (avgSentenceLength > 10 && avgSentenceLength < 100) score += 0.2;
    if (length < 50 || length > 3000) score -= 0.2;

    return Math.max(0, Math.min(1, score));
  }
}
